{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 Fletcher\n",
    "## Part 4 Extracting for Named Entity\n",
    "\n",
    "### Explored frontier technology news in decades from New York Times API. Applied unsupervised machine learning methods to cluster different topics in tech news and compared the popular fields in time series.\n",
    "\n",
    "\n",
    "Technology Trend in New York Times: rest API | NLP | LDA | Word2Vec | PCA | Clustering | MongoDB\n",
    "<br>\n",
    "• Summarized top topics from frontier technology news in 1945-2017 using New York Times API.\n",
    "<br>\n",
    "• Extracted public relationship network of aimed company from mass data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from time import sleep\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tech1945_2017.plk','rb') as f:\n",
    "    t=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpdocs_yearly = []\n",
    "for i in range(1945,2018):\n",
    "    lpdocs_yearly.append([doc['lead_paragraph'] for doc in t[i] if doc['lead_paragraph']!=None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# lpdocs_yearly is list of lists of strings\n",
    "print(type(lpdocs_yearly), type(lpdocs_yearly[0]), type(lpdocs_yearly[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# encoding text as UTF-8 \n",
    "lp2016 = lpdocs_yearly[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp2016 = lpdocs_yearly[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp2016 = [lp.encode(encoding='UTF-8',errors='strict') for lp in lp2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lp = lpdocs_yearly[-1]\n",
    "\n",
    "lp = [l.encode(encoding='UTF-8',errors='strict') for l in lp]\n",
    "\n",
    "relations = []\n",
    "for doc in lp:\n",
    "\n",
    "    nes = nlp.semgrex(doc, pattern='{tag: NNP}', filter=False)\n",
    "    for s in nes['sentences']:\n",
    "        del s['length']\n",
    "        nnp_objs = s.values()\n",
    "\n",
    "        # skip sentences that contain no NNP\n",
    "        if len(nnp_objs) == 0:\n",
    "            continue\n",
    "\n",
    "        # sort nnp by their begin position\n",
    "        nnp_objs = sorted(nnp_objs, key=lambda k: k['begin'])\n",
    "\n",
    "        nnps = []\n",
    "        i = 0\n",
    "        while i < len(nnp_objs):\n",
    "\n",
    "            curr_nnp = nnp_objs[i]['text']\n",
    "\n",
    "            # combine consecutive NNP\n",
    "            j = i\n",
    "            while j < len(nnp_objs)-1:\n",
    "                if nnp_objs[j]['end'] == nnp_objs[j+1]['begin']:\n",
    "                    curr_nnp += ' ' + nnp_objs[j+1]['text']\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "            if len(curr_nnp) > 0:\n",
    "                nnps.append(curr_nnp) \n",
    "\n",
    "            i = j+1\n",
    "\n",
    "        if len(nnps) > 0:\n",
    "            relations.append(nnps)\n",
    "with open('relations2017.plk','wb') as f:\n",
    "    pickle.dump(relations,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Google', 'Facebook'],\n",
       " ['Politwoops', 'API'],\n",
       " ['Thursday', 'Duchenne'],\n",
       " ['Rebuild', 'Mailbox'],\n",
       " ['Adam Bosworth'],\n",
       " ['Firefox'],\n",
       " ['MANICAS', 'Peter T.'],\n",
       " ['Peter T. Manicas',\n",
       "  'Queens College',\n",
       "  'City University',\n",
       "  'New York',\n",
       "  'University',\n",
       "  'Hawaii',\n",
       "  'Manoa',\n",
       "  'December',\n",
       "  'Honolulu'],\n",
       " ['Kenoe Kauanoe', 'Ti', 'Manicas', 'Teddy', 'Theodore'],\n",
       " ['Tautasi']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
